# Sensor-Fusion-and-Object-Tracking
1.  Write a short recap of the four tracking steps and what you implemented there (EKF, track management, data association, camera-lidar sensor fusion). Which results did you achieve? Which part of the project was most difficult for you to complete, and why?
2.  Do you see any benefits in camera-lidar fusion compared to lidar-only tracking (in theory and in your concrete results)?
3.  Which challenges will a sensor fusion system face in real-life scenarios? Did you see any of these challenges in the project?
4.  Can you think of ways to improve your tracking results in the future?
   
1.  Recap: The Extended Kalman Filter is a method of state estimation that accounts for nonlinear process and/or measurement models, as opposed to the Kalman Filter, which only accounts for linear process and measurement models. Since vehicles on the road regularly accelerate and brake, it is better to use an Extended Kalman Filter. Our EKF predicts the state, which includes position and velocity, of detected objects and updates the state using incoming measurements from both lidar and camera. It linearized the system using a first-order Taylor expansion at each time step.
The track management system is a system that determines when it is worth spending computational resources toward tracking an object or not, based on whether the object stays in our sensor’s field of view or not. It sets any newly detected track to “tentative” and continues to update its score based on subsequent measurements. As the track score increases, indicating consistent detection and tracking of the object, its state may transition to "confirmed". If an object is consistently missed or goes out of the sensor's field of view, its score decreases. When the score drops below a certain threshold, or if other conditions such as excessive estimation uncertainty are met, the track may be removed or "deleted" from active tracking. This ensures that the system focuses on tracking relevant and consistent objects while ignoring sporadic or irrelevant detections.
The association step involved matching incoming, new measurements from sensors to existing tracks. In our implementation we employed gating to associate measurements with tracks. We set up an association matrix, where the rows represented tracks and the columns represented measurements. The matrix entries were Mahalanobis distances between tracks and measurements. We employ the Mahalanobis distance as the distance metric between tracks and measurements as opposed to other metrics because it includes information about the measurement uncertainty and thus encapsulates a more robust method of association. In the implementation we utilized a nested for loop that crafted a list of Mahalanobis distances from one track to every measurement. It repeated it for each track. When an MHD did not meet the gating threshold, an infinity was appended to the lists. The lists were then combined into a numpy matrix, which would be the association matrix.
The camera-lidar sensor fusion allowed the system to achieve redundancy through the use of two sensors, each with its strengths an weaknesses. While the camera cannot see well in the dark, it can provide texture details and color information. The lidar also gives depth information. Overall, this redundant system through sensor fusion allows for higher confidence of tracking objects.
Through the implementation of the steps, we were able to achieve a high performing tracking system with an RMSE of x before adding camera measurments and y after adding camera measurements, which indicates a robust tracking system. The system confirmed tracks can be seen in the attached movie.
The most difficult part of the project for me to complete was understanding the flow of the code base, for example where the track instance was initialized, which informed my method of implementation in all scripts.

2.  Yes, while the code was progressing through frames, there were less false positives, as indicated by red tracking boxes. Also the RMSE value was lower, indicating a more robust tracking system.
3.  Sensor fusion systems could face many challenges in real-life situations such as computational overhead due to the real time nature of autonomous driving, calibration issues of sensors where misalignment could lead to inaccurate measurements, and data synchronization where both sensors fuse data from the same time stamp. I did not notice any of these challenges in the project.
4.  Yes, we could use a data-driven model such as a Neural Network or a different type of filter such as a particle filter instead of Kalman filter for state estimation. We could incorporate additional sensors such as radar. Also, we use a fusion system that determines when to use camera data based on the driving situation to optimize computational load and to optimize when which sensor is best utilized
